{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn pandas numpy joblib"
      ],
      "metadata": {
        "id": "rAgGN0zS2EW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcfNhy8S0zH5"
      },
      "outputs": [],
      "source": [
        "# Trains calibrated Linear SVM models (class-balanced) for\n",
        "# VitalityCategory and StressCategory.\n",
        "# Uses 8 raw bands + 16 indices + 8 log1p bands.\n",
        "# Prints full holdout results and generates thesis-ready plots.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Tuple, List\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, balanced_accuracy_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from joblib import dump"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# CONFIG\n",
        "# ======================\n",
        "DATA_PATH = \"/content/Data_To_Train_3_vitality.xlsx\"  # .xlsx/.xls/.csv all OK\n",
        "TEST_SIZE = 0.25\n",
        "RANDOM_STATE = 42\n",
        "DO_CV = False\n",
        "CV_SPLITS = 3\n",
        "CV_REPEATS = 5\n",
        "\n",
        "# === Band mapping (your sensor) ===\n",
        "BANDS = [f\"B{i}\" for i in range(1, 9)]\n",
        "BLUE, GREEN1, GREEN, RED, RE, NIR = \"B2\", \"B3\", \"B4\", \"B6\", \"B7\", \"B8\"\n",
        "TARGETS = [\"VitalityCategory\", \"StressCategory\"]\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Helpers\n",
        "# ======================\n",
        "def safe_nd(a, b, eps=1e-9): return (a - b) / (a + b + eps)\n",
        "def safe_ratio(a, b, eps=1e-9): return a / (b + eps)\n",
        "def safe_sqrt(x): return np.sqrt(np.maximum(x, 0.0))\n",
        "\n",
        "def load_table(path: str) -> pd.DataFrame:\n",
        "    ext = Path(path).suffix.lower()\n",
        "    return pd.read_excel(path) if ext in [\".xlsx\", \".xls\"] else pd.read_csv(path)"
      ],
      "metadata": {
        "id": "kig76NhU1M_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# Feature computation\n",
        "# ======================\n",
        "def compute_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"8 raw bands + 16 indices + 8 logs\"\"\"\n",
        "    B2, B3, B4, B6, B7, B8 = [df[b].astype(float) for b in [BLUE, GREEN1, GREEN, RED, RE, NIR]]\n",
        "    X_raw = df[BANDS].apply(pd.to_numeric, errors=\"coerce\").copy()\n",
        "\n",
        "    X_idx = pd.DataFrame(index=df.index)\n",
        "    X_idx[\"NDVI\"] = safe_nd(B8, B6)\n",
        "    X_idx[\"EVI\"] = 2.5*(B8 - B6)/(B8 + 6*B6 - 7.5*B2 + 1.0 + 1e-9)\n",
        "    X_idx[\"NDRE\"] = safe_nd(B8, B7)\n",
        "    X_idx[\"CIre\"] = safe_ratio(B8, B7) - 1\n",
        "    X_idx[\"GNDVI\"] = safe_nd(B8, B4)\n",
        "    X_idx[\"SAVI\"] = (B8 - B6)*(1+0.5)/(B8 + B6 + 0.5 + 1e-9)\n",
        "    X_idx[\"MSAVI2\"] = (2*B8 + 1 - safe_sqrt((2*B8 + 1)**2 - 8*(B8 - B6))) / 2\n",
        "    X_idx[\"RDVI\"] = (B8 - B6) / (safe_sqrt(B8 + B6) + 1e-9)\n",
        "    X_idx[\"PRI\"] = safe_nd(B3, B4)\n",
        "    X_idx[\"VARI\"] = (B4 - B6)/(B4 + B6 - B2 + 1e-9)\n",
        "    X_idx[\"CIgreen\"] = safe_ratio(B8, B4) - 1\n",
        "    X_idx[\"MTCI\"] = (B8 - B7)/(B7 - B6 + 1e-9)\n",
        "    X_idx[\"NDWI\"] = safe_nd(B4, B8)\n",
        "    X_idx[\"ratio_B7_B6\"] = safe_ratio(B7, B6)\n",
        "    X_idx[\"ratio_B6_B8\"] = safe_ratio(B6, B8)\n",
        "    X_idx[\"ratio_B2_B6\"] = safe_ratio(B2, B6)\n",
        "\n",
        "    X_logs = pd.DataFrame(index=df.index)\n",
        "    for b in BANDS:\n",
        "        X_logs[f\"log1p_{b}\"] = np.log1p(df[b].astype(float).clip(lower=0))\n",
        "\n",
        "    X = pd.concat([X_raw, X_idx, X_logs], axis=1)\n",
        "    X = X.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "    return X\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Model + Evaluation\n",
        "# ======================\n",
        "@dataclass\n",
        "class ModelResult:\n",
        "    target: str\n",
        "    accuracy: float\n",
        "    macro_f1: float\n",
        "    bal_acc: float\n",
        "    report: str\n",
        "    conf_mat: np.ndarray\n",
        "    classes: List[str]\n",
        "    features: List[str]\n",
        "\n",
        "def make_model() -> Pipeline:\n",
        "    base = LinearSVC(C=1.0, class_weight=\"balanced\", random_state=RANDOM_STATE)\n",
        "    cal = CalibratedClassifierCV(base, cv=3, method=\"sigmoid\")\n",
        "    return Pipeline([(\"scaler\", StandardScaler()), (\"cal\", cal)])"
      ],
      "metadata": {
        "id": "i8JaZ6Pq1JcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# Visualization\n",
        "# ======================\n",
        "def plot_confusion_matrix(cm, classes, target_name, outdir):\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\",\n",
        "                xticklabels=classes, yticklabels=classes)\n",
        "    plt.title(f\"{target_name} - Confusion Matrix\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outdir / f\"{target_name}_confusion_matrix.png\", dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def _extract_linear_coefs_from_calibrated(cal_obj):\n",
        "    \"\"\"\n",
        "    Return (n_classes, n_features) coefficients by averaging across\n",
        "    calibrated folds. Works for CalibratedClassifierCV over LinearSVC.\n",
        "    \"\"\"\n",
        "    coefs = []\n",
        "    if hasattr(cal_obj, \"calibrated_classifiers_\") and cal_obj.calibrated_classifiers_:\n",
        "        for cc in cal_obj.calibrated_classifiers_:\n",
        "            est = getattr(cc, \"estimator\", None)\n",
        "            if est is not None and hasattr(est, \"coef_\"):\n",
        "                c = np.asarray(est.coef_)\n",
        "                if c.ndim == 1:\n",
        "                    c = c.reshape(1, -1)\n",
        "                coefs.append(c)\n",
        "    elif hasattr(cal_obj, \"base_estimator_\") and hasattr(cal_obj.base_estimator_, \"coef_\"):\n",
        "        c = np.asarray(cal_obj.base_estimator_.coef_)\n",
        "        if c.ndim == 1:\n",
        "            c = c.reshape(1, -1)\n",
        "        coefs.append(c)\n",
        "\n",
        "    if not coefs:\n",
        "        return None\n",
        "    return np.mean(np.stack(coefs, axis=0), axis=0)  # average over folds\n",
        "\n",
        "\n",
        "def plot_feature_importance(model, features, target_name, outdir):\n",
        "    cal = model.named_steps.get(\"cal\", None)\n",
        "    if cal is None:\n",
        "        print(f\"⚠️ No 'cal' step in pipeline for {target_name}; skipping feature importance.\")\n",
        "        return\n",
        "\n",
        "    coefs = _extract_linear_coefs_from_calibrated(cal)\n",
        "    if coefs is None:\n",
        "        print(f\"⚠️ Could not extract coefficients for {target_name}; skipping feature importance.\")\n",
        "        return\n",
        "\n",
        "    # Importance = mean absolute coefficient across classes\n",
        "    importance = np.mean(np.abs(coefs), axis=0)\n",
        "    imp_df = pd.DataFrame({\"Feature\": features, \"Importance\": importance}).sort_values(\n",
        "        \"Importance\", ascending=False\n",
        "    ).head(20)\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.barplot(y=\"Feature\", x=\"Importance\", data=imp_df, palette=\"mako\")\n",
        "    plt.title(f\"{target_name} - Top 20 Feature Importances (|coef|, averaged over calibrated folds)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outdir / f\"{target_name}_feature_importance.png\", dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_class_report(rep_dict, target_name, outdir):\n",
        "    metrics = pd.DataFrame(rep_dict).T.drop([\"accuracy\"], errors=\"ignore\")\n",
        "    metrics = metrics.dropna(subset=[\"precision\"], how=\"all\")\n",
        "    metrics = metrics.loc[~metrics.index.str.contains(\"avg\", case=False)]\n",
        "    metrics[[\"precision\", \"recall\", \"f1-score\"]].plot(kind=\"bar\", figsize=(7,5))\n",
        "    plt.title(f\"{target_name} - Per-class Performance\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outdir / f\"{target_name}_class_report.png\", dpi=300)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "hsXLpB9l1GdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# Training + Evaluation\n",
        "# ======================\n",
        "def fit_eval_one_target(X, y_series, target_name):\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(y_series.astype(str))\n",
        "    class_names = list(le.classes_)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        "    )\n",
        "\n",
        "    model = make_model()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    mf1 = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
        "    bacc = balanced_accuracy_score(y_test, y_pred)\n",
        "    rep_text = classification_report(y_test, y_pred, target_names=class_names, zero_division=0)\n",
        "    rep_dict = classification_report(y_test, y_pred, target_names=class_names, output_dict=True, zero_division=0)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\n==== {target_name} RESULTS (holdout set) ====\")\n",
        "    print(f\"Accuracy: {acc:.4f} | Balanced Acc: {bacc:.4f} | Macro-F1: {mf1:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(rep_text)\n",
        "    print(\"\\nConfusion matrix (rows=true, cols=pred):\")\n",
        "    print(pd.DataFrame(cm, index=class_names, columns=class_names))\n",
        "\n",
        "    # === Visualization ===\n",
        "    outdir = Path(\"lsvm_outputs\") / f\"plots_{target_name}\"\n",
        "    outdir.mkdir(parents=True, exist_ok=True)\n",
        "    plot_confusion_matrix(cm, class_names, target_name, outdir)\n",
        "    plot_feature_importance(model, X.columns.tolist(), target_name, outdir)\n",
        "    plot_class_report(rep_dict, target_name, outdir)\n",
        "\n",
        "    # Save per-target holdout predictions\n",
        "    y_test_df = pd.DataFrame({\n",
        "        f\"{target_name}_true\": le.inverse_transform(y_test),\n",
        "        f\"{target_name}_pred\": le.inverse_transform(y_pred)\n",
        "    }, index=X_test.index)\n",
        "    y_test_df.to_csv(outdir / f\"holdout_predictions_{target_name}.csv\")\n",
        "\n",
        "    res = ModelResult(target_name, acc, mf1, bacc, rep_text, cm, class_names, X.columns.tolist())\n",
        "    return res, model, le"
      ],
      "metadata": {
        "id": "XTs8EidH1Dy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# MAIN\n",
        "# ======================\n",
        "def main():\n",
        "    print(\"Loading data...\")\n",
        "    df = load_table(DATA_PATH)\n",
        "    need = set(BANDS + TARGETS)\n",
        "    miss = need - set(df.columns)\n",
        "    if miss:\n",
        "        raise ValueError(f\"Missing required columns: {sorted(miss)}\")\n",
        "\n",
        "    df = df.dropna(subset=TARGETS).reset_index(drop=True)\n",
        "    X_full = compute_features(df)\n",
        "    df = df.loc[X_full.index].reset_index(drop=True)\n",
        "    X_full = X_full.reset_index(drop=True)\n",
        "    print(f\"Total features constructed: {X_full.shape[1]}\")\n",
        "\n",
        "    results = {}\n",
        "    Path(\"lsvm_outputs\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for tgt in TARGETS:\n",
        "        print(f\"\\nTraining {tgt} model...\")\n",
        "        res, model, le = fit_eval_one_target(X_full, df[tgt], tgt)\n",
        "        results[tgt] = res\n",
        "        dump(model, f\"lsvm_outputs/linear_svm_cal_{tgt}.joblib\")\n",
        "        dump(le, f\"lsvm_outputs/label_encoder_{tgt}.joblib\")\n",
        "\n",
        "    # Summary results\n",
        "    summary_df = pd.DataFrame([{\n",
        "        \"Target\": tgt,\n",
        "        \"Accuracy\": results[tgt].accuracy,\n",
        "        \"MacroF1\": results[tgt].macro_f1,\n",
        "        \"BalancedAcc\": results[tgt].bal_acc\n",
        "    } for tgt in results])\n",
        "    summary_df.to_csv(\"lsvm_outputs/holdout_summary.csv\", index=False)\n",
        "\n",
        "    print(\"\\n===== SUMMARY =====\")\n",
        "    print(summary_df.to_string(index=False))\n",
        "\n",
        "    # Summary bar plot\n",
        "    summary_df.set_index(\"Target\")[[\"Accuracy\", \"MacroF1\", \"BalancedAcc\"]].plot(kind=\"bar\", figsize=(6,4))\n",
        "    plt.title(\"Linear SVM Model Performance Comparison\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"lsvm_outputs/model_comparison_summary.png\", dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    print(\"\\n✅ All outputs (models, CSVs, and plots) saved in ./lsvm_outputs/\")"
      ],
      "metadata": {
        "id": "R4cIg5e80_B4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Jj_q9xa50-N1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}