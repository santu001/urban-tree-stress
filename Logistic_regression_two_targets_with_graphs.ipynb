{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MSvqoQ6z-28"
      },
      "outputs": [],
      "source": [
        "# Trains class-balanced multinomial Logistic Regression models\n",
        "# and generates explanatory plots for thesis (feature importance, confusion matrix, etc.)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Tuple, List\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, balanced_accuracy_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from joblib import dump"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# CONFIG\n",
        "# ======================\n",
        "DATA_PATH = \"/content/Data_To_Train_3_vitality.xlsx\"\n",
        "TEST_SIZE = 0.25\n",
        "RANDOM_STATE = 42\n",
        "DO_CV = False\n",
        "CV_SPLITS = 3\n",
        "CV_REPEATS = 5\n",
        "\n",
        "# Band mapping per your sensor\n",
        "BANDS = [f\"B{i}\" for i in range(1, 9)]\n",
        "COASTAL, BLUE, GREEN1, GREEN, YELLOW, RED, RE, NIR = \\\n",
        "    \"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"B8\"\n",
        "TARGETS = [\"VitalityCategory\", \"StressCategory\"]"
      ],
      "metadata": {
        "id": "S1FEnT_k0nTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# Utility Functions\n",
        "# ======================\n",
        "def load_table(path: str) -> pd.DataFrame:\n",
        "    ext = Path(path).suffix.lower()\n",
        "    if ext in [\".xlsx\", \".xls\"]:\n",
        "        return pd.read_excel(path)\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "\n",
        "def compute_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    eps = 1e-9\n",
        "    X_raw = df[BANDS].apply(pd.to_numeric, errors=\"coerce\").copy()\n",
        "    X_idx = pd.DataFrame(index=df.index)\n",
        "\n",
        "    # Normalized differences\n",
        "    X_idx[\"NDVI\"]   = (df[NIR] - df[RED])   / (df[NIR] + df[RED] + eps)\n",
        "    X_idx[\"GNDVI\"]  = (df[NIR] - df[GREEN]) / (df[NIR] + df[GREEN] + eps)\n",
        "    X_idx[\"NDRE\"]   = (df[NIR] - df[RE])    / (df[NIR] + df[RE] + eps)\n",
        "    X_idx[\"NDWI\"]   = (df[GREEN] - df[NIR]) / (df[GREEN] + df[NIR] + eps)\n",
        "\n",
        "    # EVI\n",
        "    X_idx[\"EVI\"]    = 2.5*(df[NIR]-df[RED])/(df[NIR] + 6*df[RED] - 7.5*df[BLUE] + 1.0 + eps)\n",
        "\n",
        "    # SAVI, MSAVI2, RDVI\n",
        "    L = 0.5\n",
        "    X_idx[\"SAVI\"]   = (df[NIR]-df[RED])*(1+L)/(df[NIR]+df[RED]+L+eps)\n",
        "    term = (2*df[NIR] + 1.0)\n",
        "    X_idx[\"MSAVI2\"] = 0.5*(term - np.sqrt(np.maximum(term**2 - 8*(df[NIR] - df[RED]), 0) + eps))\n",
        "    X_idx[\"RDVI\"]   = (df[NIR] - df[RED]) / np.sqrt(df[NIR] + df[RED] + eps)\n",
        "\n",
        "    # Chlorophyll/structure & ratios\n",
        "    X_idx[\"CIre\"]      = (df[NIR] / (df[RE] + eps))   - 1.0\n",
        "    X_idx[\"CIgreen\"]   = (df[NIR] / (df[GREEN] + eps)) - 1.0\n",
        "    X_idx[\"MTCI\"]      = (df[NIR] - df[RE]) / (df[RE] - df[RED] + eps)\n",
        "    X_idx[\"PRI\"]       = (df[GREEN1] - df[GREEN]) / (df[GREEN1] + df[GREEN] + eps)\n",
        "    X_idx[\"VARI\"]      = (df[GREEN] - df[RED]) / (df[GREEN] + df[RED] - df[BLUE] + eps)\n",
        "    X_idx[\"Ratio_RE_Red\"]   = df[RE]  / (df[RED] + eps)\n",
        "    X_idx[\"Ratio_Red_NIR\"]  = df[RED] / (df[NIR] + eps)\n",
        "    X_idx[\"Ratio_Blue_Red\"] = df[BLUE]/ (df[RED] + eps)\n",
        "\n",
        "    X_logs = pd.DataFrame(index=df.index)\n",
        "    for b in BANDS:\n",
        "        X_logs[f\"log1p_{b}\"] = np.log1p(df[b].astype(float).clip(lower=0))\n",
        "\n",
        "    X = pd.concat([X_raw, X_idx, X_logs], axis=1)\n",
        "    X = X.replace([np.inf, -np.inf], np.nan).dropna(axis=0, how=\"any\")\n",
        "    return X\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelResult:\n",
        "    target: str\n",
        "    accuracy: float\n",
        "    macro_f1: float\n",
        "    bal_acc: float\n",
        "    report: str\n",
        "    conf_mat: np.ndarray\n",
        "    classes: List[str]\n",
        "    feature_names: List[str]\n",
        "    coefs: np.ndarray\n",
        "\n",
        "\n",
        "def make_model() -> Pipeline:\n",
        "    \"\"\"StandardScaler -> LogisticRegression (multinomial, class-balanced)\"\"\"\n",
        "    clf = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"logreg\", LogisticRegression(\n",
        "            multi_class=\"multinomial\",\n",
        "            solver=\"saga\",\n",
        "            class_weight=\"balanced\",\n",
        "            C=1.0,\n",
        "            max_iter=5000,\n",
        "            random_state=RANDOM_STATE,\n",
        "            n_jobs=-1\n",
        "        ))\n",
        "    ])\n",
        "    return clf"
      ],
      "metadata": {
        "id": "CzGTO6Vl0knN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# Visualization Helpers\n",
        "# ======================\n",
        "def plot_confusion_matrix(cm, classes, target_name, outdir):\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\",\n",
        "                xticklabels=classes, yticklabels=classes)\n",
        "    plt.title(f\"{target_name} - Confusion Matrix\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outdir / f\"{target_name}_confusion_matrix.png\", dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_feature_importance(coefs, feature_names, target_name, outdir):\n",
        "    # Use mean abs(coef) across classes for multinomial\n",
        "    importance = np.mean(np.abs(coefs), axis=0)\n",
        "    imp_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importance})\n",
        "    imp_df = imp_df.sort_values(\"Importance\", ascending=False).head(20)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.barplot(y=\"Feature\", x=\"Importance\", data=imp_df, palette=\"viridis\")\n",
        "    plt.title(f\"{target_name} - Top 20 Feature Importances (|standardized coef|)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outdir / f\"{target_name}_feature_importance.png\", dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_class_report(report_dict, target_name, outdir):\n",
        "    metrics = pd.DataFrame(report_dict).T.drop([\"accuracy\"], errors=\"ignore\")\n",
        "    metrics = metrics.dropna(subset=[\"precision\"], how=\"all\")\n",
        "    metrics = metrics.loc[~metrics.index.str.contains(\"avg\", case=False)]\n",
        "\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    metrics[[\"precision\", \"recall\", \"f1-score\"]].plot(kind=\"bar\")\n",
        "    plt.title(f\"{target_name} - Per-class Performance\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outdir / f\"{target_name}_class_report.png\", dpi=300)\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "QaswyW5W0gvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# Main Train + Evaluate\n",
        "# ======================\n",
        "def fit_eval_one_target(X: pd.DataFrame, y_series: pd.Series, target_name: str):\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(y_series.astype(str))\n",
        "    class_names = list(le.classes_)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        "    )\n",
        "\n",
        "    model = make_model()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    mf1 = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
        "    bacc = balanced_accuracy_score(y_test, y_pred)\n",
        "\n",
        "    rep = classification_report(y_test, y_pred, target_names=class_names, output_dict=True, zero_division=0)\n",
        "    cm  = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\n==== {target_name} (holdout) ====\")\n",
        "    print(f\"Accuracy: {acc:.4f} | Balanced Acc: {bacc:.4f} | Macro-F1: {mf1:.4f}\")\n",
        "\n",
        "    coefs = model.named_steps[\"logreg\"].coef_\n",
        "    feature_names = X.columns.tolist()\n",
        "\n",
        "    res = ModelResult(target_name, acc, mf1, bacc,\n",
        "                      classification_report(y_test, y_pred, target_names=class_names, zero_division=0),\n",
        "                      cm, class_names, feature_names, coefs)\n",
        "\n",
        "    # === Visualization ===\n",
        "    outdir = Path(\"logreg_outputs\") / f\"plots_{target_name}\"\n",
        "    outdir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    plot_confusion_matrix(cm, class_names, target_name, outdir)\n",
        "    plot_feature_importance(coefs, feature_names, target_name, outdir)\n",
        "    plot_class_report(rep, target_name, outdir)\n",
        "\n",
        "    return res, model, le"
      ],
      "metadata": {
        "id": "MM4DtM5A0dIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"Loading data...\")\n",
        "    df = load_table(DATA_PATH)\n",
        "\n",
        "    need = set(BANDS + TARGETS)\n",
        "    miss = need - set(df.columns)\n",
        "    if miss:\n",
        "        raise ValueError(f\"Missing required columns: {sorted(miss)}\")\n",
        "\n",
        "    df = df.dropna(subset=TARGETS).reset_index(drop=True)\n",
        "    X_full = compute_features(df)\n",
        "    df = df.loc[X_full.index].reset_index(drop=True)\n",
        "    X_full = X_full.reset_index(drop=True)\n",
        "    print(f\"Total features built: {X_full.shape[1]}\")\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for tgt in TARGETS:\n",
        "        print(f\"\\nTraining {tgt} model...\")\n",
        "        res, model, le = fit_eval_one_target(X_full, df[tgt], tgt)\n",
        "        results[tgt] = res\n",
        "\n",
        "        dump(model, f\"logreg_outputs/logreg_multinomial_{tgt}.joblib\")\n",
        "        dump(le, f\"logreg_outputs/label_encoder_{tgt}.joblib\")\n",
        "\n",
        "    # Combined summary plot\n",
        "    summary_df = pd.DataFrame([{\n",
        "        \"Target\": tgt,\n",
        "        \"Accuracy\": results[tgt].accuracy,\n",
        "        \"MacroF1\": results[tgt].macro_f1,\n",
        "        \"BalancedAcc\": results[tgt].bal_acc\n",
        "    } for tgt in results])\n",
        "    summary_df.to_csv(\"logreg_outputs/holdout_summary.csv\", index=False)\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    summary_df.set_index(\"Target\")[[\"Accuracy\", \"MacroF1\", \"BalancedAcc\"]].plot(kind=\"bar\")\n",
        "    plt.title(\"Model Performance Comparison\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"logreg_outputs/model_comparison_summary.png\", dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    print(\"\\nAll outputs and plots saved in ./logreg_outputs/\")"
      ],
      "metadata": {
        "id": "oCXXIF5H0ZvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "S_gBtq9W0Xol"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}